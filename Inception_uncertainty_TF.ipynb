{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inception with uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train_class = pd.read_csv('data/train_split_v4.csv', sep=' ')#, header=True)#, names=['patientID', 'image_path', 'class'])\n",
    "test_class = pd.read_csv('data/test_split_v4.csv', sep=' ')#, header=True)#, names=['patientID', 'image_path', 'class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pictures (data_frame,data_dir,channels):\n",
    "    import numpy as np\n",
    "    from PIL import Image\n",
    "    pics = []\n",
    "    for img in data_frame['image_path'][:1500].values:\n",
    "        if channels ==3:\n",
    "            pics.append(np.array(Image.open(data_dir + img))[:, :,:3])\n",
    "        else:\n",
    "            pics.append(np.array(Image.open(data_dir + img))[:, :,0])\n",
    "\n",
    "    return np.array(pics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images: (1500, 200, 200, 3) (1500, 200, 200, 3)\n"
     ]
    }
   ],
   "source": [
    "num_channels = 3\n",
    "X_train = load_pictures(train_class,\"data/train/\",channels = num_channels)\n",
    "X_test = load_pictures(test_class,\"data/test/\",channels = num_channels)\n",
    "print(\"Number of images:\", X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape X: 1500 1500,  disease_ID (Y): 1500 1500\n"
     ]
    }
   ],
   "source": [
    "from codvidutils.imageproc import map_categorical\n",
    "import numpy as np\n",
    "values_dict = {'COVID-19': 1,'pneumonia': 2,'normal': 0}\n",
    "test_class['class_categorical'] = test_class['class'].apply(map_categorical, args=(values_dict,))\n",
    "train_class['class_categorical'] = train_class['class'].apply(map_categorical, args=(values_dict,))\n",
    "diseaseID_train = np.asarray(train_class[\"class_categorical\"][:1500])\n",
    "diseaseID_test = np.asarray(test_class[\"class_categorical\"][:1500])\n",
    "print('shape X: {} {},  disease_ID (Y): {} {}'.format(X_train.shape[0], X_test.shape[0], diseaseID_train.shape[0], diseaseID_test.shape[0] ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 1398, 1: 87, 2: 15})\n",
      "Counter({0: 1398, 1: 87, 2: 15})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "counter = Counter (diseaseID_train)\n",
    "print(counter)\n",
    "#dicto = {2: 500, 0: 500, 1:counter[1]}\n",
    "dicto = counter\n",
    "print(dicto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1500, 120000)\n",
      "(1500,)\n",
      "(1500, 200, 200, 3)\n"
     ]
    }
   ],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "#X = X.reshape(X,X.shape[0],-1)\n",
    "X_train = X_train.reshape(X_train.shape[0],-1)\n",
    "print(X_train.shape)\n",
    "under = RandomUnderSampler(sampling_strategy =dicto)\n",
    "X_train, diseaseID_train = under.fit_resample(X_train, diseaseID_train)\n",
    "# summarize class distribution\n",
    "if num_channels == 3:\n",
    "    X_train = X_train.reshape(X_train.shape[0],200,200,3)\n",
    "else:\n",
    "    X_train = X_train.reshape(X_train.shape[0],200,200)\n",
    "    \n",
    "print(diseaseID_train.shape)\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal train:  (1398,)\n",
      "Pneumonia train:  (15,)\n",
      "COVID train:  (87,)\n",
      "*******************************************************\n",
      "Normal test:  (885,)\n",
      "Pneumonia test:  (591,)\n",
      "COVID test:  (24,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Normal train: \",diseaseID_train[diseaseID_train==0].shape)\n",
    "print(\"Pneumonia train: \",diseaseID_train[diseaseID_train==2].shape)\n",
    "print(\"COVID train: \",diseaseID_train[diseaseID_train==1].shape)\n",
    "print(\"*******************************************************\")\n",
    "print(\"Normal test: \",diseaseID_test[diseaseID_test==0].shape)\n",
    "print(\"Pneumonia test: \",diseaseID_test[diseaseID_test==2].shape)\n",
    "print(\"COVID test: \",diseaseID_test[diseaseID_test==1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adding_images(X, Y,strides= 5):#kernel =None ):\n",
    "    from codvidutils import nwpic as nw\n",
    "    new_X = nw.new_pictures_arrays(X[Y==1],strides)\n",
    "    X = X[:,10:190,10:190]\n",
    "    new_Y = np.ones(new_X.shape[0])\n",
    "    X = np.concatenate([X,new_X],axis=0)\n",
    "    Y = np.concatenate([Y,new_Y],axis=0)\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from codvidutils import nwpic as nw\n",
    "\"\"\"\n",
    "News images to train \n",
    "\"\"\"\n",
    "X_train, diseaseID_train = adding_images(X_train, diseaseID_train, strides=10 )\n",
    "X_test, diseaseID_test = adding_images(X_test, diseaseID_test, strides=10 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test/255\n",
    "X_train = X_train/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = np.copy(diseaseID_train)\n",
    "del diseaseID_train\n",
    "Y_train[Y_train==2]=0\n",
    "Y_test = np.copy(diseaseID_test)\n",
    "Y_test[Y_test==2]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inception_block_R(previous_layer, p_drop,p_l2,filters =[64,128,32,32]):\n",
    "    from tensorflow.keras.layers import concatenate, Conv2D, Dropout, MaxPooling2D\n",
    "    from tensorflow.keras.regularizers import l2\n",
    "    \n",
    "    previous_drop = Dropout(rate = p_drop)(previous_layer, training=True)\n",
    "    rama_1 = Conv2D(filters=filters[0], kernel_size=(1,1), activation='relu',\n",
    "                             bias_regularizer = l2(p_l2), kernel_regularizer=l2(p_l2))(previous_drop)\n",
    "    #rama_1 = Dropout(rate = p_drop)(rama_1)\n",
    "    \n",
    "    rama_2 = Conv2D(filters=filters[1], kernel_size=(1,1), activation='relu',\n",
    "                             bias_regularizer = l2(p_l2), kernel_regularizer=l2(p_l2))(previous_drop)\n",
    "    rama_2 = Dropout(rate = p_drop)(rama_2, training=True)\n",
    "    rama_2 = Conv2D(filters=filters[1], kernel_size=(3,3), activation='relu',padding=\"same\",\n",
    "                             bias_regularizer = l2(p_l2), kernel_regularizer=l2(p_l2))(rama_2)\n",
    "    #rama_2 = Dropout(rate = p_drop)(rama_2)\n",
    "    \n",
    "    rama_3 = Conv2D(filters=filters[2], kernel_size=(1,1), activation='relu',\n",
    "                             bias_regularizer = l2(p_l2), kernel_regularizer=l2(p_l2))(previous_drop)\n",
    "    rama_3 = Dropout(rate = p_drop)(rama_3, training=True)\n",
    "    rama_3 = Conv2D(filters=filters[2], kernel_size=(5,5), activation='relu',padding=\"same\",\n",
    "                             bias_regularizer = l2(p_l2), kernel_regularizer=l2(p_l2))(rama_3)\n",
    "    #rama_3 = Dropout(rate = p_drop)(rama_3)\n",
    "    \n",
    "    rama_4 = MaxPooling2D(pool_size=(3,3),padding=\"same\",strides=1)(previous_layer)\n",
    "    rama_4 = Dropout(rate = p_drop)(rama_4, training=True)\n",
    "    rama_4 = Conv2D(filters=filters[3], kernel_size=(1,1), activation='relu',padding=\"same\",\n",
    "                             bias_regularizer = l2(p_l2), kernel_regularizer=l2(p_l2))(rama_4)\n",
    "    #rama_4 = Dropout(rate = p_drop)(rama_4)\n",
    "    \n",
    "    inception = concatenate([rama_1, rama_2,rama_3,rama_4])\n",
    "    return inception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Incep_6 archiquecture using Functional api\n",
    "\"\"\"\n",
    "def deep_inception_R(p_drop,p_l2):\n",
    "    from tensorflow.keras.layers import MaxPooling2D, Dense, Flatten, GlobalMaxPooling2D, Dropout, Input,concatenate\n",
    "    from tensorflow.keras.models import Model\n",
    "    from tensorflow.keras.regularizers import l2\n",
    "    input_layer = Input(shape= (180,180,3,))\n",
    "    #input_drop = Dropout(rate = p_drop)(input_layer)\n",
    "    \n",
    "    inception_1 = inception_block_R(input_layer,p_drop,p_l2)\n",
    "    pool_1 = MaxPooling2D()(inception_1)\n",
    "    \n",
    "    inception_2 = inception_block_R(pool_1,p_drop,p_l2)\n",
    "    pool_2 = MaxPooling2D()(inception_2)\n",
    "    \n",
    "    inception_3 = inception_block_R(pool_2,p_drop,p_l2)\n",
    "    pool_3 = MaxPooling2D()(inception_3)\n",
    "    \n",
    "    inception_4 = inception_block_R(pool_3,p_drop,p_l2)\n",
    "    pool_4 = MaxPooling2D()(inception_4)\n",
    "    \n",
    "    inception_5 = inception_block_R(pool_4,p_drop,p_l2)\n",
    "    pool_5 = MaxPooling2D()(inception_5)\n",
    "    \n",
    "    inception_6 = inception_block_R(pool_5,p_drop,p_l2)\n",
    "    \n",
    "    gobal = GlobalMaxPooling2D()(inception_6)\n",
    "    gobal = Dropout(rate= p_drop)(gobal, training=True)\n",
    "    \n",
    "    dense = Dense(units=256,activation=\"relu\",bias_regularizer = l2(p_l2), kernel_regularizer=l2(p_l2))(gobal)\n",
    "    dense = Dropout(rate= p_drop)(dense, training=True)\n",
    "    dense = Dense(units=64,activation=\"relu\",bias_regularizer = l2(p_l2), kernel_regularizer=l2(p_l2))(dense)\n",
    "    dense = Dropout(rate= p_drop)(dense, training=True)\n",
    "    out_layer= Dense(units =1, activation= \"sigmoid\",bias_regularizer = l2(p_l2), kernel_regularizer=l2(p_l2))(dense)\n",
    "    \n",
    "    deep = Model(inputs=input_layer, outputs=out_layer)\n",
    "    return deep\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape X: (2283, 180, 180, 3),  Y: (2283,) \n",
      " Test shape X: (1716, 180, 180, 3), Y: (1716,)\n"
     ]
    }
   ],
   "source": [
    "print('Train shape X: {},  Y: {} \\n Test shape X: {}, Y: {}'.format(X_train.shape, Y_train.shape, X_test.shape, Y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0.38107752956636004, 1: 0.6189224704336399}\n"
     ]
    }
   ],
   "source": [
    "from codvidutils.utils import weigths2loss\n",
    "dic_weights = weigths2loss(Y_train)\n",
    "print(dic_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05 5e-05\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n",
      "train number 0 ****************************************************************************\n",
      "opened strategy\n",
      "Time paralisis  1.0696301460266113\n",
      "opened model\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "compiled\n",
      "Time paralisis  3.8815362453460693\n",
      "Train on 2283 samples, validate on 1716 samples\n",
      "Epoch 1/5\n",
      "INFO:tensorflow:batch_all_reduce: 78 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:batch_all_reduce: 78 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\n",
      "2272/2283 [============================>.] - ETA: 0s - loss: 0.8279 - acc: 0.6171 - mse: 0.2380\n",
      "Epoch 00001: val_acc improved from -inf to 0.86014, saving model to Unc_0_trained.hdf5\n",
      "2283/2283 [==============================] - 41s 18ms/sample - loss: 0.8285 - acc: 0.6159 - mse: 0.2383 - val_loss: 0.7001 - val_acc: 0.8601 - val_mse: 0.1780\n",
      "Epoch 2/5\n",
      "2272/2283 [============================>.] - ETA: 0s - loss: 0.8144 - acc: 0.6197 - mse: 0.2361\n",
      "Epoch 00002: val_acc did not improve from 0.86014\n",
      "2283/2283 [==============================] - 16s 7ms/sample - loss: 0.8146 - acc: 0.6189 - mse: 0.2362 - val_loss: 0.7668 - val_acc: 0.8590 - val_mse: 0.2147\n",
      "Epoch 3/5\n",
      "2272/2283 [============================>.] - ETA: 0s - loss: 0.7960 - acc: 0.6193 - mse: 0.2315\n",
      "Epoch 00003: val_acc did not improve from 0.86014\n",
      "2283/2283 [==============================] - 16s 7ms/sample - loss: 0.7962 - acc: 0.6189 - mse: 0.2316 - val_loss: 0.6687 - val_acc: 0.8572 - val_mse: 0.1717\n",
      "Epoch 4/5\n",
      "2272/2283 [============================>.] - ETA: 0s - loss: 0.7044 - acc: 0.6981 - mse: 0.1944\n",
      "Epoch 00004: val_acc did not improve from 0.86014\n",
      "2283/2283 [==============================] - 16s 7ms/sample - loss: 0.7046 - acc: 0.6978 - mse: 0.1944 - val_loss: 0.7563 - val_acc: 0.6142 - val_mse: 0.2218\n",
      "Epoch 5/5\n",
      "2272/2283 [============================>.] - ETA: 0s - loss: 0.6109 - acc: 0.7738 - mse: 0.1578\n",
      "Epoch 00005: val_acc did not improve from 0.86014\n",
      "2283/2283 [==============================] - 16s 7ms/sample - loss: 0.6107 - acc: 0.7740 - mse: 0.1578 - val_loss: 0.6827 - val_acc: 0.6731 - val_mse: 0.1948\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n",
      "train number 1 ****************************************************************************\n",
      "opened strategy\n",
      "Time paralisis  0.004847049713134766\n",
      "opened model\n",
      "compiled\n",
      "Time paralisis  3.8811044692993164\n",
      "Train on 2283 samples, validate on 1716 samples\n",
      "Epoch 1/5\n",
      "INFO:tensorflow:batch_all_reduce: 78 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\n",
      "INFO:tensorflow:batch_all_reduce: 78 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\n",
      "2272/2283 [============================>.] - ETA: 0s - loss: 0.8307 - acc: 0.6144 - mse: 0.2391\n",
      "Epoch 00001: val_acc improved from -inf to 0.86014, saving model to Unc_1_trained.hdf5\n",
      "2283/2283 [==============================] - 38s 17ms/sample - loss: 0.8318 - acc: 0.6128 - mse: 0.2397 - val_loss: 0.7001 - val_acc: 0.8601 - val_mse: 0.1777\n",
      "Epoch 2/5\n",
      "2272/2283 [============================>.] - ETA: 0s - loss: 0.8167 - acc: 0.6193 - mse: 0.2369\n",
      "Epoch 00002: val_acc did not improve from 0.86014\n",
      "2283/2283 [==============================] - 16s 7ms/sample - loss: 0.8170 - acc: 0.6189 - mse: 0.2370 - val_loss: 0.7219 - val_acc: 0.8601 - val_mse: 0.1923\n",
      "Epoch 3/5\n",
      "2272/2283 [============================>.] - ETA: 0s - loss: 0.8056 - acc: 0.6193 - mse: 0.2354"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "#import tensorflow.keras\n",
    "from time import time\n",
    "#import keras.backend as K\n",
    "import tensorflow as tf\n",
    "prob = []     \n",
    "hist_for =[]\n",
    "T = 4    \n",
    "pdro = 0.05\n",
    "pl2 = 0.000050\n",
    "\n",
    "#***********************************************************\n",
    "t0 = time()\n",
    "print(pdro, pl2)\n",
    "#strategy = tf.distribute.OneDeviceStrategy(device=\"/gpu:0\")\n",
    "for i in range(T):\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "    with strategy.scope():\n",
    "    #for i in range(T):\n",
    "        print(\"train number\" , i, \"****************************************************************************\")\n",
    "        filepath=\"Unc_\"+str(i)+\"_trained.hdf5\"\n",
    "        print(\"opened strategy\")\n",
    "        #tf.keras.backend.clear_session()\n",
    "\n",
    "        print(\"Time paralisis \", time()-t0)\n",
    "        model = deep_inception_R(p_drop = pdro, p_l2 = pl2)\n",
    "\n",
    "        print(\"opened model\")\n",
    "        checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "        ASG = Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, amsgrad=True)\n",
    "\n",
    "        model.compile(optimizer=ASG, loss='binary_crossentropy', metrics=['acc',\"mse\"])\n",
    "        print(\"compiled\")\n",
    "        print(\"Time paralisis \", time()-t0)\n",
    "\n",
    "        #history = model.fit(X_train, Y_train, epochs = 30, batch_size=32, callbacks= [checkpoint],verbose=2, class_weight=dic_weights,validation_data=(X_test, Y_test))\n",
    "        history = model.fit(X_train, Y_train, epochs=5, batch_size=32, callbacks= [checkpoint],verbose=1,validation_data=(X_test, Y_test))\n",
    "        prob += [model.predict(X_test)]\n",
    "        hist_for += [history]\n",
    "        t0 = time()\n",
    "        del strategy # <----cuando uno se enfada pasa esto\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = []\n",
    "\n",
    "l = 0.01\n",
    "for _ in range(T):\n",
    "    probs += [model.predict(X_test)]\n",
    "    \n",
    "predictive_mean = np.mean(prob, axis=0)\n",
    "predictive_variance = np.var(prob, axis=0)\n",
    "tau = l**2 * (1 - pdro) / (2 * T * pl2)\n",
    "predictive_variance += tau**-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tau =   (1 - pd) / (2 * 5 * pl2)\n",
    "tau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf = pandas.DataFrame([])\n",
    "for H in hist_for:\n",
    "    df = pandas.DataFrame(H.history)\n",
    "    hdf = pandas.concat([hdf,df],axis=0)#, ingore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf.to_csv(\"histories_drop.txt\", index= False, sep = \" \")#, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Normal train: \",Y_train[Y_train==0].shape)\n",
    "print(\"COVID train: \",Y_train[Y_train==1].shape)\n",
    "print(\"*******************************************************\")\n",
    "print(\"Normal test: \",Y_test[Y_test==0].shape)\n",
    "print(\"COVID test: \",Y_test[Y_test==1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig,ax = plt.subplots(nrows=1,ncols=1,figsize=(8,8))#,6\n",
    "plt.style.use('seaborn-deep') \n",
    "plt.grid(True)\n",
    "name=\"acc\"\n",
    "i=1\n",
    "for h in hist_for:\n",
    "    plt.plot(h.history[name] ,label=\"Itera \"+str(i))\n",
    "    i=i+1\n",
    "    plt.plot(h.history[\"val_\"+name], color= \"orange\" ,label=\"Testing data set\")\n",
    "    \n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend(loc='center')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_drop = 0.05\n",
    "p_l2 = 0.000050\n",
    "tau = (1 - p_drop) / (2 * 3 * p_l2)\n",
    "Dsis = 1 / tau\n",
    "print(tau)\n",
    "print(Dsis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Means & desvs\n",
    "\"\"\"\n",
    "prob = np.asarray(prob)\n",
    "Mean = np.mean(prob,axis =0)\n",
    "Dsta = np.std(prob,axis = 0)\n",
    "unc = Dsta + Dsis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def integ(x,m,d):\n",
    "    from math import pi, sqrt, exp\n",
    "    return 1 / ( sqrt(2 * pi) * d) * exp(-(m-x)**2 / (2 * d**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import normalize\n",
    "for P in prob:\n",
    "    cm = confusion_matrix(Y_test, P.round())\n",
    "    cm_norm = normalize(cm, norm = 'l1')\n",
    "    print(cm, \"\\n\", cm_norm)\n",
    "    \n",
    "print(\"******* MEAN values******\")\n",
    "cm = confusion_matrix(Y_test, Mean.round())\n",
    "cm_norm = normalize(cm, norm = 'l1')\n",
    "print(cm, \"\\n\", cm_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def integraciones(integ, intervalos,A, B):\n",
    "    from scipy.integrate import quad\n",
    "    from numpy import inf\n",
    "    intervalos[intervalos==0], intervalos[intervalos==1] =-inf, inf\n",
    "    \n",
    "    proba = np.ones([1,len(intervalos)-1])\n",
    "    for j in range(len(A)):\n",
    "        probabilidad = []\n",
    "        for it in range(len(intervalos)-1):\n",
    "\n",
    "            I = quad(integ,intervalos[it], intervalos[it+1], args = (A[j], B[j]) )\n",
    "            probabilidad += [I]\n",
    "\n",
    "        probabilidad = np.array(probabilidad)[:,0]\n",
    "        probabilidad = probabilidad.reshape([1,len(intervalos)-1])\n",
    "\n",
    "        proba = np.concatenate([proba,probabilidad],axis=0)\n",
    "    return proba[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_cuentas = {0: [], 1:[] }\n",
    "dic_dest = {0: [], 1:[] }\n",
    "intervalos = np.linspace(0,1, num = 38)\n",
    "A = Mean[Y_test==0]\n",
    "B = unc[Y_test==0]\n",
    "print(len(A),len(B))\n",
    "prob = integraciones(integ,intervalos,A,B)\n",
    "cuentas = np.nansum(prob,axis=0)\n",
    "varianza = np.nansum((1-prob)*prob,axis=0)\n",
    "dic_cuentas[0] = cuentas\n",
    "dic_dest[0] = np.sqrt(varianza)\n",
    "A = Mean[Y_test==1]\n",
    "B = unc[Y_test==1]\n",
    "print(len(A),len(B))\n",
    "prob = integraciones(integ,intervalos,A,B)\n",
    "cuentas = np.nansum(prob,axis=0)\n",
    "varianza = np.nansum((1-prob)*prob,axis=0)\n",
    "dic_cuentas[1] = cuentas\n",
    "dic_dest[1] = np.sqrt(varianza)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "pred = prob[0]#model.predict(X_test)\n",
    "fig,ax = plt.subplots(nrows=1,ncols=1,figsize=(8,8))# 6,6\n",
    "plt.style.use('seaborn-deep')\n",
    "plt.hist(pred[(Y_test==0)],38,histtype='step',color='darkorange',lw=2,label= \"No covid\" )\n",
    "plt.hist(pred[Y_test==1],38,histtype='step',color='blue',lw=1 ,label=\"Covid\")\n",
    "plt.ylabel('Number of events')\n",
    "plt.xlabel('Score')\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()\n",
    "#plt.savefig(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig,ax = plt.subplots(nrows=1,ncols=1,figsize=(8,8))# 6,6\n",
    "intervalos=intervalos[:-1]\n",
    "intervalos += intervalos[1]/2\n",
    "plt.style.use('seaborn-deep')\n",
    "plt.errorbar(intervalos, dic_cuentas[0] , yerr=dic_dest[0],lw=2, label =\"No covid\")\n",
    "plt.errorbar(intervalos, dic_cuentas[1] , yerr=dic_dest[1],lw=2, label =\"Covid\")\n",
    "plt.ylabel('Number of counts')\n",
    "plt.xlabel('Score')\n",
    "plt.legend(loc=\"upper center\")\n",
    "#plt.savefig(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "fpr, tpr, thresholds = roc_curve(Y_test, Mean)\n",
    "fpr_U, tpr_U, thr = roc_curve(Y_test,Mean+unc)\n",
    "fpr_L, tpr_L, thr = roc_curve(Y_test,Mean-unc)\n",
    "roc_auc = auc(fpr,tpr)\n",
    "roc_auc_U = auc(fpr_U,tpr_U)\n",
    "roc_auc_L = auc(fpr_L,tpr_L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(nrows=1,ncols=1,figsize=(8,8))#,6\n",
    "plt.style.use('seaborn-deep') \n",
    "#plt.title(\"Simulation\", weight=\"bold\", x=0.50)\n",
    "plt.grid(True)\n",
    "#yticks(np.arange(0.97, 1, step=0.005))\n",
    "lw = 2\n",
    "plt.plot(fpr, tpr, color='orange',lw=3, label='ROC-curve (AUC = %0.3f)' % (roc_auc))\n",
    "plt.plot(fpr_U, tpr_U, color='blue',lw=1, label='ROC-curve Upper (AUC = %0.3f)' % (roc_auc_U))\n",
    "plt.plot(fpr_L, tpr_L, color='blue',lw=1, label='ROC-curve Lower (AUC = %0.3f)' % (roc_auc_L))\n",
    "\n",
    "\n",
    "\n",
    "plt.ylabel('True Positie Rate (TPR)')\n",
    "plt.xlabel('False Positive Rate (FPR)')\n",
    "\n",
    "#plt.title(\"Simulation Preliminary\", color = '#e159e1',loc=\"left\", weight=\"bold\", size=\"large\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
